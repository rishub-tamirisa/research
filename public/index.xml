<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Rishub Tamirisa</title>
    <link>http://localhost:1313/hugo-website/</link>
    <description>Recent content on Rishub Tamirisa</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language><atom:link href="http://localhost:1313/hugo-website/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning</title>
      <link>http://localhost:1313/hugo-website/papers/paper3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/hugo-website/papers/paper3/</guid>
      <description>Links Paper (CVPR 2024) Paper (ICML 2023 Workshop Ver.) Abstract Standard federated learning approaches suffer when client data distributions have sufficient heterogeneity. Recent methods addressed the client data heterogeneity issue via personalized federated learning (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients&amp;rsquo; local data distributions. Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.</description>
    </item>
    
    <item>
      <title>Location</title>
      <link>http://localhost:1313/hugo-website/location/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/hugo-website/location/</guid>
      <description>Mailing and office addresses at the University of Place.</description>
    </item>
    
    <item>
      <title>Office Hours</title>
      <link>http://localhost:1313/hugo-website/officehours/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/hugo-website/officehours/</guid>
      <description>Schedule and location for office hours.</description>
    </item>
    
    <item>
      <title>The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning</title>
      <link>http://localhost:1313/hugo-website/papers/paper2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/hugo-website/papers/paper2/</guid>
      <description>Links Paper Project Page (with code &amp;amp; data) Abstract The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use.</description>
    </item>
    
    <item>
      <title>Toward Robust Unlearning for LLMs</title>
      <link>http://localhost:1313/hugo-website/papers/paper1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/hugo-website/papers/paper1/</guid>
      <description>Abstract Recent rapid advances in AI enabled by large language models (LLMs) have raised widespread concerns regarding their potential for malicious misuse. While traditional open-source software has long established mechanisms for combating such adversarial behavior, systems involving large neural networks are nontrivial to interpret—let alone intervene on—for safe use. Various alignment methods have been proposed to steer model responses towards a desired &amp;ldquo;safe&amp;rdquo; output distribution. However, these techniques are superficial and remain susceptible to adversarial prompting, and can be undone entirely with supervised fine-tuning.</description>
    </item>
    
  </channel>
</rss>
